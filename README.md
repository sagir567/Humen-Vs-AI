# Ai_vs_human

## Description of Work:
The task involved analyzing a dataset containing texts generated by humans and texts generated by AI. The goal was to build a classification model capable of distinguishing between human-generated and AI-generated texts based on various features extracted from the text data. The features included text length, sentiment scores, punctuation count, count of linking words, count of synonyms, and word frequency distribution.

Initial Attempts:
1. Data Preprocessing:
The text data was tokenized, and preprocessing steps like removing stopwords, non-alphabetic characters, and stemming were applied. Also we removed duplicatet texts.
2. Feature Engineering:
Features such as text length, sentiment scores, punctuation count, count of linking words, count of synonyms, and word frequency distribution were extracted from the text data.
3. Model Training:
We tried the three models and finally we arrived at logistic regression, this is the model that brought us the best results. We will see screen shots at the end of the explanations

## Improvements Made:
1. Feature Selection:
Initially, a large number of features were extracted, including word features for the top 500 most common words. To improve model performance and reduce complexity, feature selection techniques were applied to select the most relevant features. The length of the text, sentiment scores, the most common word in each text and the number of times it appears
2. Model Selection:
In addition to Logistic Regression, other models such as Linear Regression and SoftMax Regression were explored and compared to determine the most suitable model for the task.
3. Evaluation:
The models were evaluated based on metrics such as accuracy, classification report, and class-wise metrics to gain insights into their performance and identify areas for improvement.
4. Data Augmentation:
To address potential issues of overfitting or underfitting, techniques such as data augmentation could be explored to increase the diversity of the training data and improve model generalization.

## Results:
The classification models achieved varying levels of accuracy in distinguishing between human-generated and AI-generated texts.
Detailed evaluation metrics such as precision, recall, and F1-score provided insights into the performance of each model.
Comparison with a simple linear or logistic regression (or softmax) model helped assess the effectiveness of the selected approach 

## Comparison with Simple Models:
The selected classification models outperformed simple linear or logistic regression models in terms of accuracy and other evaluation metrics.
The inclusion of additional features such as sentiment scores, punctuation count, and count of synonyms improved the classification performance compared to simple linear models.

## Conclusion:
The work demonstrates the effectiveness of feature engineering and advanced classification techniques in distinguishing between human-generated and AI-generated texts.
Further experimentation with feature selection, model tuning, and data augmentation techniques could potentially enhance the performance of the classification models and address challenges such as overfitting or underfitting.




